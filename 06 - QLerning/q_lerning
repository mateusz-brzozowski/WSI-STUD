import gym
import random
import numpy as np
from statistics import mean
from multiprocessing.pool import Pool
from functools import partial
from itertools import repeat

ALL_EPISODES = [1000, 10000, 50000, 100000, 250000]
LEARNING_RATE = 0.8
MAX_STEPS = 200
GAMMA = 0.9

MAX_EPSILON = 1.0
MIN_EPSILON = 0.001
DECAY_RATE = 0.00005


def default_reward(env, end_point):
    if env.desc[end_point] == b'G':
        return 1
    return 0

def first_reward(env, end_point):
    if env.desc[end_point] == b'G':
        return 1
    elif env.desc[end_point] == b'H':
        return -1
    return 0

def second_reward(env, end_point):
    if env.desc[end_point] == b'G':
        return 5
    elif env.desc[end_point] == b'H':
        return -1
    return 0

REWARD_FUNCTIONS = [second_reward]

def q_lerning(env, episodes, reward_function):
    qtable = np.zeros((env.observation_space.n, env.action_space.n))
    epsilon = MAX_EPSILON
    for episode in range(episodes):
        fields = env.reset()
        done = False
        for _ in range(MAX_STEPS):
            if random.uniform(0, 1) > epsilon:
                move = np.argmax(qtable[fields,:])
            else:
                move = env.action_space.sample()
            new_fields, _, done, _ = env.step(move)
            end_point = divmod(new_fields, 8)
            reward = reward_function(env, end_point)
            qtable[fields, move] = qtable[fields, move] + LEARNING_RATE * (reward + GAMMA * np.max(qtable[new_fields, :]) - qtable[fields, move])
            fields = new_fields
            if done == True:
                break
        epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE*episode)
    return qtable


def test_q_lerning(env, qtable):
    env.reset()
    total_succes = 0
    for _ in range(1000):
        fields = env.reset()
        done = False
        for _ in range(MAX_STEPS):
            move = np.argmax(qtable[fields,:])
            new_fields, reward, done, _ = env.step(move)
            if done == True:
                if reward == 1.0:
                    total_succes += 1
                break
            fields = new_fields
    env.close()
    return total_succes / 10

def run_q_lerning(total_episodes, reward_function):
    env = gym.make("FrozenLake8x8-v1")
    qtable = q_lerning(env, total_episodes, reward_function)
    return test_q_lerning(env, qtable)


def main():
    for reward_function in REWARD_FUNCTIONS:
        print(reward_function.__name__)
        print('episodes | avg success')
        print('---------|------------')
        for total_episodes in ALL_EPISODES:
            with Pool() as pool:
                repeat_funtion = partial(run_q_lerning, reward_function=reward_function)
                total_succes = pool.map(repeat_funtion, repeat(total_episodes, 30))
                print(f'{total_episodes:8} | {mean(total_succes):.2f}' + "%")


if __name__ == "__main__":
    main()