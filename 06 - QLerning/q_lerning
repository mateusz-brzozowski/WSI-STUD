import gym
import random
import numpy as np
from functools import partial

env = gym.make("FrozenLake8x8-v1")
ALL_EPISODES = [1000]

def default_reward(state):
    row, col = divmod(state, 8)
    if env.desc[row, col] == b'G':
        return 1
    return 0

def first_reward(state):
    row, col = divmod(state, 8)
    if env.desc[row, col] == b'G':
        return 1
    elif env.desc[row, col] == b'H':
        return -1
    return 0

def second_reward(state):
    row, col = divmod(state, 8)
    if env.desc[row, col] == b'G':
        return 10
    elif env.desc[row, col] == b'H':
        return -5
    else:
        return -1

REWARD_FUNCTIONS = [default_reward, first_reward, second_reward]

def q_lerning(qtable, total_episodes, learning_rate, max_steps, gamma, epsilon, max_epsilon, min_epsilon, decay_rate):
    total_rewards = 0
    for episode in range(total_episodes):
        state = env.reset()
        done = False
        for _ in range(max_steps):
            exp_exp_tradeoff = random.uniform(0, 1)
            if exp_exp_tradeoff > epsilon:
                action = np.argmax(qtable[state,:])
            else:
                action = env.action_space.sample()
            new_state, _, done, _ = env.step(action)
            reward = default_reward(new_state)
            qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])
            state = new_state
            if done == True:
                break
        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)
    return total_rewards


def test_q_lerning(qtable, max_steps):
    env.reset()
    total_succes = 0
    for _ in range(1000):
        state = env.reset()
        done = False
        for _ in range(max_steps):
            action = np.argmax(qtable[state,:])
            new_state, reward, done, _ = env.step(action)
            if done:
                if reward == 1.0:
                    total_succes += 1
                break
            state = new_state
    env.close()
    return total_succes

def main():
    total_episodes = 1000        # Total episodes
    learning_rate = 0.8           # Learning rate
    max_steps = 400                # Max steps per episode
    gamma = 0.9                  # Discounting rate

    # Exploration parameters
    epsilon = 1.0                 # Exploration rate
    max_epsilon = 1.0             # Exploration probability at start
    min_epsilon = 0.001            # Minimum exploration probability
    decay_rate = 0.00005             # Exponential decay rate for exploration prob

    qtable = np.zeros((env.observation_space.n, env.action_space.n))

    for reward_function in REWARD_FUNCTIONS:
        print(reward_function.__name__)
        for total_episodes in ALL_EPISODES:
            q_lerning(qtable, total_episodes, learning_rate, max_steps, gamma, epsilon, max_epsilon, min_epsilon, decay_rate)
            total_succes = test_q_lerning(qtable, max_steps)
            print(f'{((total_succes/1000) * 100):.6f}' + "%")

if __name__ == "__main__":
    main()